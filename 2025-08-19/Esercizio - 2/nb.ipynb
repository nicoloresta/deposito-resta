{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53bc150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "31828501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9471 entries, 0 to 9470\n",
      "Data columns (total 17 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Date           9357 non-null   object \n",
      " 1   Time           9357 non-null   object \n",
      " 2   CO(GT)         9357 non-null   object \n",
      " 3   PT08.S1(CO)    9357 non-null   float64\n",
      " 4   NMHC(GT)       9357 non-null   float64\n",
      " 5   C6H6(GT)       9357 non-null   object \n",
      " 6   PT08.S2(NMHC)  9357 non-null   float64\n",
      " 7   NOx(GT)        9357 non-null   float64\n",
      " 8   PT08.S3(NOx)   9357 non-null   float64\n",
      " 9   NO2(GT)        9357 non-null   float64\n",
      " 10  PT08.S4(NO2)   9357 non-null   float64\n",
      " 11  PT08.S5(O3)    9357 non-null   float64\n",
      " 12  T              9357 non-null   object \n",
      " 13  RH             9357 non-null   object \n",
      " 14  AH             9357 non-null   object \n",
      " 15  Unnamed: 15    0 non-null      float64\n",
      " 16  Unnamed: 16    0 non-null      float64\n",
      "dtypes: float64(10), object(7)\n",
      "memory usage: 1.2+ MB\n",
      "None\n",
      "            Date      Time CO(GT)  PT08.S1(CO)  NMHC(GT) C6H6(GT)  \\\n",
      "0     10/03/2004  18.00.00    2,6       1360.0     150.0     11,9   \n",
      "1     10/03/2004  19.00.00      2       1292.0     112.0      9,4   \n",
      "2     10/03/2004  20.00.00    2,2       1402.0      88.0      9,0   \n",
      "3     10/03/2004  21.00.00    2,2       1376.0      80.0      9,2   \n",
      "4     10/03/2004  22.00.00    1,6       1272.0      51.0      6,5   \n",
      "...          ...       ...    ...          ...       ...      ...   \n",
      "9466         NaN       NaN    NaN          NaN       NaN      NaN   \n",
      "9467         NaN       NaN    NaN          NaN       NaN      NaN   \n",
      "9468         NaN       NaN    NaN          NaN       NaN      NaN   \n",
      "9469         NaN       NaN    NaN          NaN       NaN      NaN   \n",
      "9470         NaN       NaN    NaN          NaN       NaN      NaN   \n",
      "\n",
      "      PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  \\\n",
      "0            1046.0    166.0        1056.0    113.0        1692.0   \n",
      "1             955.0    103.0        1174.0     92.0        1559.0   \n",
      "2             939.0    131.0        1140.0    114.0        1555.0   \n",
      "3             948.0    172.0        1092.0    122.0        1584.0   \n",
      "4             836.0    131.0        1205.0    116.0        1490.0   \n",
      "...             ...      ...           ...      ...           ...   \n",
      "9466            NaN      NaN           NaN      NaN           NaN   \n",
      "9467            NaN      NaN           NaN      NaN           NaN   \n",
      "9468            NaN      NaN           NaN      NaN           NaN   \n",
      "9469            NaN      NaN           NaN      NaN           NaN   \n",
      "9470            NaN      NaN           NaN      NaN           NaN   \n",
      "\n",
      "      PT08.S5(O3)     T    RH      AH  Unnamed: 15  Unnamed: 16  \n",
      "0          1268.0  13,6  48,9  0,7578          NaN          NaN  \n",
      "1           972.0  13,3  47,7  0,7255          NaN          NaN  \n",
      "2          1074.0  11,9  54,0  0,7502          NaN          NaN  \n",
      "3          1203.0  11,0  60,0  0,7867          NaN          NaN  \n",
      "4          1110.0  11,2  59,6  0,7888          NaN          NaN  \n",
      "...           ...   ...   ...     ...          ...          ...  \n",
      "9466          NaN   NaN   NaN     NaN          NaN          NaN  \n",
      "9467          NaN   NaN   NaN     NaN          NaN          NaN  \n",
      "9468          NaN   NaN   NaN     NaN          NaN          NaN  \n",
      "9469          NaN   NaN   NaN     NaN          NaN          NaN  \n",
      "9470          NaN   NaN   NaN     NaN          NaN          NaN  \n",
      "\n",
      "[9471 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('./rsc/AirQualityUCI.csv', sep=';')\n",
    "print(dataset.info())\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ca0e0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9357 entries, 0 to 9356\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Date     9357 non-null   object \n",
      " 1   Time     9357 non-null   object \n",
      " 2   NO2(GT)  9357 non-null   float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 292.4+ KB\n",
      "None\n",
      "            Date      Time  NO2(GT)\n",
      "0     10/03/2004  18.00.00    113.0\n",
      "1     10/03/2004  19.00.00     92.0\n",
      "2     10/03/2004  20.00.00    114.0\n",
      "3     10/03/2004  21.00.00    122.0\n",
      "4     10/03/2004  22.00.00    116.0\n",
      "...          ...       ...      ...\n",
      "9352  04/04/2005  10.00.00    190.0\n",
      "9353  04/04/2005  11.00.00    179.0\n",
      "9354  04/04/2005  12.00.00    175.0\n",
      "9355  04/04/2005  13.00.00    156.0\n",
      "9356  04/04/2005  14.00.00    168.0\n",
      "\n",
      "[9357 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Select only Date, Time, NO2 columns\n",
    "dataset = dataset[['Date', 'Time', 'NO2(GT)']].dropna()\n",
    "print(dataset.info())\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "93d85d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Date  NO2(GT)\n",
      "0    2004-03-10 18:00:00    113.0\n",
      "1    2004-03-10 19:00:00     92.0\n",
      "2    2004-03-10 20:00:00    114.0\n",
      "3    2004-03-10 21:00:00    122.0\n",
      "4    2004-03-10 22:00:00    116.0\n",
      "...                  ...      ...\n",
      "9352 2005-04-04 10:00:00    190.0\n",
      "9353 2005-04-04 11:00:00    179.0\n",
      "9354 2005-04-04 12:00:00    175.0\n",
      "9355 2005-04-04 13:00:00    156.0\n",
      "9356 2005-04-04 14:00:00    168.0\n",
      "\n",
      "[9357 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a unified Date column of type Date - specificando il formato\n",
    "dataset['Date'] = pd.to_datetime(\n",
    "    dataset['Date'] + ' ' + dataset['Time'], \n",
    "    format='%d/%m/%Y %H.%M.%S'\n",
    ")\n",
    "dataset = dataset[['Date', 'NO2(GT)']]\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5875a783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Date  NO2(GT)  Global_Average  Daily_Average  \\\n",
      "0    2004-03-10 18:00:00    113.0       58.148873     108.833333   \n",
      "1    2004-03-10 19:00:00     92.0       58.148873     108.833333   \n",
      "2    2004-03-10 20:00:00    114.0       58.148873     108.833333   \n",
      "3    2004-03-10 21:00:00    122.0       58.148873     108.833333   \n",
      "4    2004-03-10 22:00:00    116.0       58.148873     108.833333   \n",
      "...                  ...      ...             ...            ...   \n",
      "9352 2005-04-04 10:00:00    190.0       58.148873     122.000000   \n",
      "9353 2005-04-04 11:00:00    179.0       58.148873     122.000000   \n",
      "9354 2005-04-04 12:00:00    175.0       58.148873     122.000000   \n",
      "9355 2005-04-04 13:00:00    156.0       58.148873     122.000000   \n",
      "9356 2005-04-04 14:00:00    168.0       58.148873     122.000000   \n",
      "\n",
      "      Weekly_Average Quality_vs_Global Quality_vs_Daily Quality_vs_Weekly  \n",
      "0          95.892157              poor             poor              poor  \n",
      "1          95.892157              poor             good              good  \n",
      "2          95.892157              poor             poor              poor  \n",
      "3          95.892157              poor             poor              poor  \n",
      "4          95.892157              poor             poor              poor  \n",
      "...              ...               ...              ...               ...  \n",
      "9352      122.000000              poor             poor              poor  \n",
      "9353      122.000000              poor             poor              poor  \n",
      "9354      122.000000              poor             poor              poor  \n",
      "9355      122.000000              poor             poor              poor  \n",
      "9356      122.000000              poor             poor              poor  \n",
      "\n",
      "[9357 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset['Global_Average'] = dataset['NO2(GT)'].mean()\n",
    "dataset['Daily_Average'] = dataset.groupby(dataset['Date'].dt.date)['NO2(GT)'].transform('mean')\n",
    "dataset['Weekly_Average'] = dataset.groupby(pd.Grouper(key='Date', freq='W'))['NO2(GT)'].transform('mean')\n",
    "\n",
    "# Classification based on different averages\n",
    "dataset['Quality_vs_Global'] = dataset['NO2(GT)'].apply(\n",
    "    lambda x: 'good' if x <= dataset['Global_Average'].iloc[0] else 'poor'\n",
    ")\n",
    "\n",
    "dataset['Quality_vs_Daily'] = dataset.apply(\n",
    "    lambda row: 'good' if row['NO2(GT)'] <= row['Daily_Average'] else 'poor', \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "dataset['Quality_vs_Weekly'] = dataset.apply(\n",
    "    lambda row: 'good' if row['NO2(GT)'] <= row['Weekly_Average'] else 'poor', \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2b43b901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality_vs_Global\n",
      "poor    6771\n",
      "good    2586\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Quality_vs_Daily\n",
      "poor    5233\n",
      "good    4124\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Quality_vs_Weekly\n",
      "poor    5415\n",
      "good    3942\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification statistics\n",
    "print(dataset['Quality_vs_Global'].value_counts(), end='\\n\\n')\n",
    "\n",
    "print(dataset['Quality_vs_Daily'].value_counts(), end='\\n\\n')\n",
    "\n",
    "print(dataset['Quality_vs_Weekly'].value_counts(), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a28dc0",
   "metadata": {},
   "source": [
    "# Dataset splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed84094a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 7485\n",
      "Test set size: 1872\n"
     ]
    }
   ],
   "source": [
    "# Preparazione delle feature\n",
    "# Estraiamo componenti temporali dalla data\n",
    "dataset['Hour'] = dataset['Date'].dt.hour\n",
    "dataset['DayOfWeek'] = dataset['Date'].dt.dayofweek\n",
    "dataset['Month'] = dataset['Date'].dt.month\n",
    "\n",
    "# Features per il modello\n",
    "X = dataset[['Hour', 'DayOfWeek', 'Month', 'NO2(GT)']]\n",
    "# X = dataset[['Date', 'NO2(GT)']]\n",
    "y = dataset['Quality_vs_Weekly']\n",
    "\n",
    "# Encoding della variabile target\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Split dei dati\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e8be3",
   "metadata": {},
   "source": [
    "# DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3f0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione e addestramento del modello Decision Tree\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predizioni e valutazione\n",
    "dt_pred = dt_model.predict(X_test)\n",
    "print(classification_report(y_test, dt_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ecea1f",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "45927819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        good       0.84      0.73      0.78       789\n",
      "        poor       0.82      0.90      0.86      1083\n",
      "\n",
      "    accuracy                           0.82      1872\n",
      "   macro avg       0.83      0.81      0.82      1872\n",
      "weighted avg       0.83      0.82      0.82      1872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "print(classification_report(y_test, lr_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d9092",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c7a7841f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        good       0.86      0.86      0.86       789\n",
      "        poor       0.90      0.90      0.90      1083\n",
      "\n",
      "    accuracy                           0.88      1872\n",
      "   macro avg       0.88      0.88      0.88      1872\n",
      "weighted avg       0.88      0.88      0.88      1872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "print(classification_report(y_test, rf_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5536c93",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "110fdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        good       0.86      0.89      0.87       789\n",
      "        poor       0.92      0.89      0.91      1083\n",
      "\n",
      "    accuracy                           0.89      1872\n",
      "   macro avg       0.89      0.89      0.89      1872\n",
      "weighted avg       0.89      0.89      0.89      1872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardizzazione delle feature (importante per MLP)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Creazione e addestramento del modello MLP\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  # Due layer nascosti con 100 e 50 neuroni\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size='auto',\n",
    "    learning_rate='constant',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predizioni e valutazione\n",
    "mlp_pred = mlp_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, mlp_pred, target_names=le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
